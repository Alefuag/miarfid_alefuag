{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car Model identification with bi-linear models (5 points)\n",
    "Images of 20 different models of cars.\n",
    "\n",
    "Autor: Alejandro Furió Agustí\n",
    "\n",
    "- Data loading\n",
    "    - Load images and labels in arrays of shape `(Nx3x224x224)` and `(Nx1)`\n",
    "    - Converto to `Tensor`, resize and normalize\n",
    "    - Organize into `Dataloader` for better trainig loop\n",
    "\n",
    "\n",
    "- Model Architecture\n",
    "    - Pre-trained Base: Utilize different sizes of ResNet model for feature extraction.\n",
    "    - Bilinear Pooling: Combine features with outer product using einops.\n",
    "    - Classification Layer: Map pooled features to num_classes using a fully connected layer.\n",
    "\n",
    "- Freeze and Unfreeze\n",
    "    - Freezing (1st step): Freeze conv models and train only new layers. `20-40 epochs`\n",
    "        - epochs: `30`\n",
    "        - learnign rate: `1e-4`\n",
    "        - StepScheduler `step_size: 10`\n",
    "    - Unfreezing (2nd step):  Unfreeze all weights and train again. `50-60 epochs`\n",
    "        - epochs: `60`\n",
    "        - learnign rate: `1e-7`\n",
    "        - ReduceLRonPlateau: `patience=5` \n",
    "\n",
    "### Results\n",
    "- Test Accuracy: 67%\n",
    "\n",
    "*To train the model, just run all the cells up to the training loop. Run the last cell to generate plots with training metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GPU\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 16,\n",
      " 'cnn_dropout': 0.2,\n",
      " 'feat_dropout': 0.5,\n",
      " 'freeze_weights': {'epochs': 30, 'lr': 0.0002, 'step_lr': 10},\n",
      " 'unfreeze_weights': {'epochs': 60, 'lr': 1e-07, 'step_lr': 25}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "def load_and_pretty_print_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "current_file_dir = Path(os.getcwd()).resolve()\n",
    "config = load_and_pretty_print_yaml(current_file_dir / 'config.yaml')\n",
    "\n",
    "\n",
    "BATCH_SIZE = config['batch_size']\n",
    "CNN_DROPOUT = config['cnn_dropout']\n",
    "FEAT_DROPOUT = config['feat_dropout']\n",
    "FREEZE_CONFIG = config['freeze_weights']\n",
    "FREEZE_CONFIG['lr'] = float(FREEZE_CONFIG['lr'])\n",
    "UNFREEZE_CONFIG = config['unfreeze_weights']\n",
    "UNFREEZE_CONFIG['lr'] = float(UNFREEZE_CONFIG['lr'])\n",
    "\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal_model = models.resnet18\n",
    "# internal_weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "\n",
    "# internal_model = models.resnet34\n",
    "# internal_weights = models.ResNet34_Weights.IMAGENET1K_V1\n",
    "\n",
    "internal_model = models.resnet50\n",
    "internal_weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "x_train = np.load('x_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_fn = internal_weights.transforms()\n",
    "convert_to_tensor = transforms.ToTensor()\n",
    "\n",
    "print('Transforms:\\n', transform_fn)\n",
    "\n",
    "def process_image(image):\n",
    "    image = convert_to_tensor(image)\n",
    "    # image = rearrange(image, 'h w c -> c h w')\n",
    "    image = transform_fn(image)\n",
    "    return image\n",
    "\n",
    "# transform images\n",
    "x_train_transformed = list(map(process_image, x_train))\n",
    "x_test_transformed = list(map(process_image, x_test))\n",
    "\n",
    "# stack images\n",
    "x_train_tensor = torch.stack(x_train_transformed) #.to(device)\n",
    "x_test_tensor = torch.stack(x_test_transformed) #.to(device)\n",
    "\n",
    "# convert labels to tensor\n",
    "y_train_tensor = torch.tensor(y_train) #.to(device)\n",
    "y_test_tensor = torch.tensor(y_test) #.to(device)\n",
    "\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_test_tensor = y_test_tensor - 1\n",
    "\n",
    "# split test set into validation and test\n",
    "x_val_tensor, x_test_tensor, y_val_tensor, y_test_tensor = train_test_split(x_test_tensor, y_test_tensor, test_size=0.6, random_state=42)\n",
    "# print('Train:', x_train_tensor.shape, y_train_tensor.shape)\n",
    "# print('Val:', x_val_tensor.shape, y_val_tensor.shape)\n",
    "# print('Test:', x_test_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_data = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = BATCH_SIZE\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1704987277512/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BiLinearModel(nn.Module):\n",
    "    def __init__(self, num_classes, internal_model=models.resnet50, internal_weights=models.ResNet50_Weights.IMAGENET1K_V1, cnn_dropout=0.0, feat_dropout=0.0):\n",
    "        super(BiLinearModel, self).__init__()\n",
    "        \n",
    "        # Load internal model\n",
    "        self.cnn1 = internal_model(weights=internal_weights)\n",
    "        self.cnn2 = internal_model(weights=internal_weights)\n",
    "        self.feature_size = self.cnn1.fc.in_features\n",
    "        # print('Feature size:', self.feature_size)\n",
    "        \n",
    "        # remove last layers\n",
    "        self.cnn1 = nn.Sequential(*list(self.cnn1.children())[:-2])\n",
    "        self.cnn2 = nn.Sequential(*list(self.cnn2.children())[:-2])\n",
    "\n",
    "        # add batchnorm\n",
    "        # self.cnn1.add_module('BatchNorm', nn.BatchNorm2d(self.feature_size))\n",
    "        # self.cnn2.add_module('BatchNorm', nn.BatchNorm2d(self.feature_size))\n",
    "        \n",
    "        # add dropout\n",
    "        self.dropout1 = nn.Dropout(cnn_dropout)\n",
    "        self.dropout2 = nn.Dropout(cnn_dropout)\n",
    "\n",
    "        self.dropout_features = nn.Dropout(feat_dropout)\n",
    "\n",
    "        # Define bilinear pooling\n",
    "        self.fc = nn.Linear(self.feature_size**2, num_classes)\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(self.feature_size**2, self.feature_size),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(self.feature_size, num_classes)\n",
    "        # )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.cnn1(x)\n",
    "        x2 = self.cnn2(x)\n",
    "        \n",
    "        # bilinear pooling with einops\n",
    "        x1 = rearrange(x1, 'b k h w -> b k (h w)')\n",
    "        x2 = rearrange(x2, 'b k h w -> b k (h w)')\n",
    "\n",
    "        # dropouts\n",
    "        x1 = self.dropout1(x1)\n",
    "        x2 = self.dropout2(x2)\n",
    "\n",
    "        x = einsum(x1, x2, 'b i j, b k j -> b i k')\n",
    "        x = rearrange(x, 'b i j -> b (i j)')\n",
    "        x = self.dropout_features(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = BiLinearModel(num_classes=20, internal_model=internal_model, internal_weights=internal_weights, cnn_dropout=CNN_DROPOUT, feat_dropout=FEAT_DROPOUT)\n",
    "model = model.to(device)\n",
    "\n",
    "in_tensor = torch.randn(1, 3, 224, 224).to(device)\n",
    "model(in_tensor).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 49/49 [00:10<00:00,  4.79it/s, accuracy=0.25, loss=1.19e+5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: Validation accuracy: \t0.3092 \t Validation loss: \t60047.5829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 49/49 [00:10<00:00,  4.85it/s, accuracy=0.562, loss=2.79e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: Validation accuracy: \t0.3684 \t Validation loss: \t59557.3814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 49/49 [00:10<00:00,  4.84it/s, accuracy=0.812, loss=1.81e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: Validation accuracy: \t0.3224 \t Validation loss: \t68806.1270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=0.75, loss=1.93e+4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: Validation accuracy: \t0.4441 \t Validation loss: \t57396.4605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=0.875, loss=9.01e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: Validation accuracy: \t0.4013 \t Validation loss: \t56868.1789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 49/49 [00:10<00:00,  4.82it/s, accuracy=0.75, loss=8.15e+3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: Validation accuracy: \t0.4441 \t Validation loss: \t58674.9615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 49/49 [00:10<00:00,  4.79it/s, accuracy=1, loss=5.34e+3]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: Validation accuracy: \t0.4737 \t Validation loss: \t41804.2382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 49/49 [00:10<00:00,  4.78it/s, accuracy=0.75, loss=5.52e+3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: Validation accuracy: \t0.4671 \t Validation loss: \t47466.6383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 49/49 [00:10<00:00,  4.77it/s, accuracy=1, loss=3.07e+3]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: Validation accuracy: \t0.4868 \t Validation loss: \t41445.7643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 49/49 [00:10<00:00,  4.79it/s, accuracy=0.938, loss=2.32e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: Validation accuracy: \t0.5066 \t Validation loss: \t38577.1810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=1, loss=1.15e+3]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: Validation accuracy: \t0.5197 \t Validation loss: \t34964.2915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 49/49 [00:10<00:00,  4.78it/s, accuracy=1, loss=353]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: Validation accuracy: \t0.5197 \t Validation loss: \t34376.1067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=1, loss=514]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: Validation accuracy: \t0.5066 \t Validation loss: \t33807.1191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 49/49 [00:10<00:00,  4.81it/s, accuracy=1, loss=408]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: Validation accuracy: \t0.5395 \t Validation loss: \t34137.0988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 49/49 [00:10<00:00,  4.83it/s, accuracy=1, loss=207]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: Validation accuracy: \t0.5296 \t Validation loss: \t33252.4807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 49/49 [00:10<00:00,  4.81it/s, accuracy=0.938, loss=182] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: Validation accuracy: \t0.5362 \t Validation loss: \t31955.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 49/49 [00:10<00:00,  4.79it/s, accuracy=0.938, loss=369] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: Validation accuracy: \t0.5263 \t Validation loss: \t31366.8643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 49/49 [00:10<00:00,  4.79it/s, accuracy=1, loss=201]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: Validation accuracy: \t0.5230 \t Validation loss: \t32246.6119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 49/49 [00:10<00:00,  4.72it/s, accuracy=0.938, loss=273] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: Validation accuracy: \t0.5428 \t Validation loss: \t31864.7297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 49/49 [00:10<00:00,  4.74it/s, accuracy=1, loss=136]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: Validation accuracy: \t0.5493 \t Validation loss: \t32014.1207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=1, loss=84.8]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: Validation accuracy: \t0.5625 \t Validation loss: \t31676.0463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=0.938, loss=231] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: Validation accuracy: \t0.5362 \t Validation loss: \t32807.0997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 49/49 [00:10<00:00,  4.79it/s, accuracy=1, loss=385]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: Validation accuracy: \t0.5263 \t Validation loss: \t32032.0593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=1, loss=269]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: Validation accuracy: \t0.5230 \t Validation loss: \t32407.0302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=1, loss=203]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: Validation accuracy: \t0.5493 \t Validation loss: \t31288.3160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=1, loss=205]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: Validation accuracy: \t0.5658 \t Validation loss: \t31612.3906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=1, loss=208]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: Validation accuracy: \t0.5461 \t Validation loss: \t32086.0992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=1, loss=361]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: Validation accuracy: \t0.5362 \t Validation loss: \t31364.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 49/49 [00:10<00:00,  4.79it/s, accuracy=1, loss=353]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: Validation accuracy: \t0.5428 \t Validation loss: \t31342.0901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|██████████| 49/49 [00:10<00:00,  4.79it/s, accuracy=1, loss=346]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: Validation accuracy: \t0.5493 \t Validation loss: \t31435.3728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/60: 100%|██████████| 49/49 [04:36<00:00,  5.63s/it, accuracy=0.812, loss=1.65e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60: Validation accuracy: \t0.4112 \t Validation loss: \t59179.2932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/60: 100%|██████████| 49/49 [04:41<00:00,  5.75s/it, accuracy=0.375, loss=1.23e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/60: Validation accuracy: \t0.4671 \t Validation loss: \t37917.9696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/60: 100%|██████████| 49/49 [04:42<00:00,  5.76s/it, accuracy=0.812, loss=9.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/60: Validation accuracy: \t0.5263 \t Validation loss: \t22235.7722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/60: 100%|██████████| 49/49 [04:42<00:00,  5.77s/it, accuracy=0.938, loss=3.72e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/60: Validation accuracy: \t0.5888 \t Validation loss: \t20098.7020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/60: 100%|██████████| 49/49 [04:42<00:00,  5.76s/it, accuracy=0.938, loss=1.85e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/60: Validation accuracy: \t0.6053 \t Validation loss: \t15325.0949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/60: 100%|██████████| 49/49 [04:42<00:00,  5.76s/it, accuracy=1, loss=733]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/60: Validation accuracy: \t0.6184 \t Validation loss: \t14052.9491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/60: 100%|██████████| 49/49 [04:41<00:00,  5.75s/it, accuracy=1, loss=524]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/60: Validation accuracy: \t0.6414 \t Validation loss: \t14308.2931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/60: 100%|██████████| 49/49 [04:42<00:00,  5.77s/it, accuracy=1, loss=497]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/60: Validation accuracy: \t0.6743 \t Validation loss: \t12864.0396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/60:  20%|██        | 10/49 [00:57<03:44,  5.76s/it, accuracy=1, loss=76.5]   "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, frozen_weights=True):\n",
    "    train_accuracy_list = []\n",
    "    val_accuracy_list = []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "            for sample in train_loader:\n",
    "                image, label = sample\n",
    "                image, label = image.to(device), label.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(image)\n",
    "                loss = criterion(outputs, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * image.size(0)\n",
    "\n",
    "                # accuracy\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                corrects = torch.sum(preds == label.data)\n",
    "                accuracy = corrects.double() / image.size(0)\n",
    "                \n",
    "                # update progress bar\n",
    "                pbar.set_postfix(loss=running_loss/len(train_loader.dataset), accuracy=accuracy.item())\n",
    "                pbar.update(1)\n",
    "        train_accuracy_list.append(accuracy.item())\n",
    "\n",
    "        # validation accuracy and loss\n",
    "        model.eval()\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sample in val_loader:\n",
    "                image, label = sample\n",
    "                image, label = image.to(device), label.to(device)\n",
    "                outputs = model(image)\n",
    "                loss = criterion(outputs, label)\n",
    "                val_loss += loss.item()*image.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                corrects += torch.sum(preds == label.data)\n",
    "                total += image.size(0)\n",
    "\n",
    "        val_accuracy = corrects.double() / total\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        if frozen_weights:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Validation accuracy: \\t{val_accuracy:.4f} \\t Validation loss: \\t{val_loss:.4f}\")\n",
    "        val_accuracy_list.append(val_accuracy.item())\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    res = {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_accuracy_list,\n",
    "        'val_accuracy': val_accuracy_list\n",
    "        }\n",
    "\n",
    "    return res\n",
    "\n",
    "# Freeze the weights of the pre-trained models\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "for param in model.cnn1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.cnn2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=FREEZE_CONFIG['lr'], momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=FREEZE_CONFIG['step_lr'], gamma=0.1)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Train the model\n",
    "train_res_freeze = train_model(model, criterion, optimizer, scheduler, num_epochs=FREEZE_CONFIG['epochs'])\n",
    "model = train_res_freeze['model']\n",
    "\n",
    "# Unfreeze the weights and train again\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=UNFREEZE_CONFIG['lr'], momentum=0.9)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=UNFREEZE_CONFIG['step_lr'], gamma=0.1)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Train the model again\n",
    "train_res_unfreeze = train_model(model, criterion, optimizer, scheduler, num_epochs=UNFREEZE_CONFIG['epochs'], frozen_weights=False)\n",
    "model = train_res_unfreeze['model']\n",
    "\n",
    "# evaluate the model\n",
    "model.eval()\n",
    "corrects = 0\n",
    "total = 0\n",
    "with tqdm(total=len(test_loader), desc=f\"Evaluating\") as pbar:\n",
    "    with torch.no_grad():\n",
    "        for sample in test_loader:\n",
    "            image, label = sample\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            outputs = model(image)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            corrects += torch.sum(preds == label.data)\n",
    "            total += image.size(0)\n",
    "            pbar.update(1)\n",
    "print(f\"Test Accuracy: {corrects.double()/total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation accuracy\n",
    "plt.plot(train_res_freeze['train_accuracy'], label='Train accuracy (Frozen weights)')\n",
    "plt.plot(train_res_freeze['val_accuracy'], label='Validation accuracy (Frozen weights)')\n",
    "plt.legend()\n",
    "plt.savefig('train_val_acc.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(train_res_unfreeze['train_accuracy'], label='Train accuracy (Unfrozen weights)')\n",
    "plt.plot(train_res_unfreeze['val_accuracy'], label='Validation accuracy (Unfrozen weights)')\n",
    "plt.legend()\n",
    "plt.savefig('train_val_acc_unfreeze.png')\n",
    "plt.close()\n",
    "\n",
    "# combine frozen and unfrozen training accuracy\n",
    "train_accuracy = train_res_freeze['train_accuracy'] + train_res_unfreeze['train_accuracy']\n",
    "val_accuracy = train_res_freeze['val_accuracy'] + train_res_unfreeze['val_accuracy']\n",
    "\n",
    "plt.plot(train_accuracy, label='Train accuracy')\n",
    "plt.plot(val_accuracy, label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('train_val_acc_combined.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilinear pooling with einops\n",
    "# x1 = rearrange(x1, 'b k h w -> b k (h w)')\n",
    "# x2 = rearrange(x2, 'b k h w -> b k (h w)')\n",
    "# x = einsum('b i j, b k j -> b i k', x1, x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
