{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 16,\n",
      " 'cnn_dropout': 0.2,\n",
      " 'feat_dropout': 0.5,\n",
      " 'freeze_weights': {'epochs': 30, 'lr': 0.0002, 'step_lr': 10},\n",
      " 'unfreeze_weights': {'epochs': 60, 'lr': 1e-07, 'step_lr': 25}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "def load_and_pretty_print_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "current_file_dir = Path(os.getcwd()).resolve()\n",
    "config = load_and_pretty_print_yaml(current_file_dir / 'config.yaml')\n",
    "\n",
    "\n",
    "BATCH_SIZE = config['batch_size']\n",
    "CNN_DROPOUT = config['cnn_dropout']\n",
    "FEAT_DROPOUT = config['feat_dropout']\n",
    "FREEZE_CONFIG = config['freeze_weights']\n",
    "FREEZE_CONFIG['lr'] = float(FREEZE_CONFIG['lr'])\n",
    "UNFREEZE_CONFIG = config['unfreeze_weights']\n",
    "UNFREEZE_CONFIG['lr'] = float(UNFREEZE_CONFIG['lr'])\n",
    "\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal_model = models.resnet18\n",
    "# internal_weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "\n",
    "# internal_model = models.resnet34\n",
    "# internal_weights = models.ResNet34_Weights.IMAGENET1K_V1\n",
    "\n",
    "internal_model = models.resnet50\n",
    "internal_weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "x_train = np.load('x_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_fn = internal_weights.transforms()\n",
    "convert_to_tensor = transforms.ToTensor()\n",
    "\n",
    "print('Transforms:\\n', transform_fn)\n",
    "\n",
    "def process_image(image):\n",
    "    image = convert_to_tensor(image)\n",
    "    # image = rearrange(image, 'h w c -> c h w')\n",
    "    image = transform_fn(image)\n",
    "    return image\n",
    "\n",
    "# transform images\n",
    "x_train_transformed = list(map(process_image, x_train))\n",
    "x_test_transformed = list(map(process_image, x_test))\n",
    "\n",
    "# stack images\n",
    "x_train_tensor = torch.stack(x_train_transformed) #.to(device)\n",
    "x_test_tensor = torch.stack(x_test_transformed) #.to(device)\n",
    "\n",
    "# convert labels to tensor\n",
    "y_train_tensor = torch.tensor(y_train) #.to(device)\n",
    "y_test_tensor = torch.tensor(y_test) #.to(device)\n",
    "\n",
    "y_train_tensor = y_train_tensor - 1\n",
    "y_test_tensor = y_test_tensor - 1\n",
    "\n",
    "# split test set into validation and test\n",
    "x_val_tensor, x_test_tensor, y_val_tensor, y_test_tensor = train_test_split(x_test_tensor, y_test_tensor, test_size=0.6, random_state=42)\n",
    "# print('Train:', x_train_tensor.shape, y_train_tensor.shape)\n",
    "# print('Val:', x_val_tensor.shape, y_val_tensor.shape)\n",
    "# print('Test:', x_test_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Data Augmentation\n",
    "\n",
    "# train_transforms = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(10),\n",
    "#     transforms.RandomCrop(32, padding=3),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# test_transforms = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_data = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = BATCH_SIZE\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1704987277512/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BiLinearModel(nn.Module):\n",
    "    def __init__(self, num_classes, internal_model=models.resnet50, internal_weights=models.ResNet50_Weights.IMAGENET1K_V1, cnn_dropout=0.0, feat_dropout=0.0):\n",
    "        super(BiLinearModel, self).__init__()\n",
    "        \n",
    "        # Load internal model\n",
    "        self.cnn1 = internal_model(weights=internal_weights)\n",
    "        self.cnn2 = internal_model(weights=internal_weights)\n",
    "        self.feature_size = self.cnn1.fc.in_features\n",
    "        # print('Feature size:', self.feature_size)\n",
    "        \n",
    "        # remove last layers\n",
    "        self.cnn1 = nn.Sequential(*list(self.cnn1.children())[:-2])\n",
    "        self.cnn2 = nn.Sequential(*list(self.cnn2.children())[:-2])\n",
    "\n",
    "        # add batchnorm\n",
    "        # self.cnn1.add_module('BatchNorm', nn.BatchNorm2d(self.feature_size))\n",
    "        # self.cnn2.add_module('BatchNorm', nn.BatchNorm2d(self.feature_size))\n",
    "\n",
    "\n",
    "        # add dropout\n",
    "        self.dropout1 = nn.Dropout(cnn_dropout)\n",
    "        self.dropout2 = nn.Dropout(cnn_dropout)\n",
    "\n",
    "        self.dropout_features = nn.Dropout(feat_dropout)\n",
    "\n",
    "        # Define bilinear pooling\n",
    "        self.fc = nn.Linear(self.feature_size**2, num_classes)\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(self.feature_size**2, self.feature_size),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(self.feature_size, num_classes)\n",
    "        # )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.cnn1(x)\n",
    "        x2 = self.cnn2(x)\n",
    "        \n",
    "        # bilinear pooling with einops\n",
    "        x1 = rearrange(x1, 'b k h w -> b k (h w)')\n",
    "        x2 = rearrange(x2, 'b k h w -> b k (h w)')\n",
    "\n",
    "        # dropouts\n",
    "        x1 = self.dropout1(x1)\n",
    "        x2 = self.dropout2(x2)\n",
    "\n",
    "        x = einsum(x1, x2, 'b i j, b k j -> b i k')\n",
    "        x = rearrange(x, 'b i j -> b (i j)')\n",
    "        x = self.dropout_features(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = BiLinearModel(num_classes=20, internal_model=internal_model, internal_weights=internal_weights, cnn_dropout=CNN_DROPOUT, feat_dropout=FEAT_DROPOUT)\n",
    "model = model.to(device)\n",
    "\n",
    "in_tensor = torch.randn(1, 3, 224, 224).to(device)\n",
    "model(in_tensor).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 49/49 [00:10<00:00,  4.79it/s, accuracy=0.25, loss=1.19e+5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: Validation accuracy: \t0.3092 \t Validation loss: \t60047.5829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 49/49 [00:10<00:00,  4.85it/s, accuracy=0.562, loss=2.79e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: Validation accuracy: \t0.3684 \t Validation loss: \t59557.3814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 49/49 [00:10<00:00,  4.84it/s, accuracy=0.812, loss=1.81e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: Validation accuracy: \t0.3224 \t Validation loss: \t68806.1270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=0.75, loss=1.93e+4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: Validation accuracy: \t0.4441 \t Validation loss: \t57396.4605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 49/49 [00:10<00:00,  4.80it/s, accuracy=0.875, loss=9.01e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: Validation accuracy: \t0.4013 \t Validation loss: \t56868.1789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30:  29%|██▊       | 14/49 [00:02<00:07,  4.83it/s, accuracy=1, loss=1.82e+3]    "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, frozen_weights=True):\n",
    "    train_accuracy_list = []\n",
    "    val_accuracy_list = []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "            for sample in train_loader:\n",
    "                image, label = sample\n",
    "                image, label = image.to(device), label.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(image)\n",
    "                loss = criterion(outputs, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * image.size(0)\n",
    "\n",
    "                # accuracy\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                corrects = torch.sum(preds == label.data)\n",
    "                accuracy = corrects.double() / image.size(0)\n",
    "                \n",
    "                # update progress bar\n",
    "                pbar.set_postfix(loss=running_loss/len(train_loader.dataset), accuracy=accuracy.item())\n",
    "                pbar.update(1)\n",
    "        train_accuracy_list.append(accuracy.item())\n",
    "\n",
    "        # validation accuracy and loss\n",
    "        model.eval()\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sample in val_loader:\n",
    "                image, label = sample\n",
    "                image, label = image.to(device), label.to(device)\n",
    "                outputs = model(image)\n",
    "                loss = criterion(outputs, label)\n",
    "                val_loss += loss.item()*image.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                corrects += torch.sum(preds == label.data)\n",
    "                total += image.size(0)\n",
    "\n",
    "        val_accuracy = corrects.double() / total\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        if frozen_weights:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Validation accuracy: \\t{val_accuracy:.4f} \\t Validation loss: \\t{val_loss:.4f}\")\n",
    "        val_accuracy_list.append(val_accuracy.item())\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    res = {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_accuracy_list,\n",
    "        'val_accuracy': val_accuracy_list\n",
    "        }\n",
    "\n",
    "    return res\n",
    "\n",
    "# Freeze the weights of the pre-trained models\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "for param in model.cnn1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.cnn2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=FREEZE_CONFIG['lr'], momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=FREEZE_CONFIG['step_lr'], gamma=0.1)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Train the model\n",
    "train_res_freeze = train_model(model, criterion, optimizer, scheduler, num_epochs=FREEZE_CONFIG['epochs'])\n",
    "model = train_res_freeze['model']\n",
    "\n",
    "# Unfreeze the weights and train again\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=UNFREEZE_CONFIG['lr'], momentum=0.9)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=UNFREEZE_CONFIG['step_lr'], gamma=0.1)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# Train the model again\n",
    "train_res_unfreeze = train_model(model, criterion, optimizer, scheduler, num_epochs=UNFREEZE_CONFIG['epochs'], frozen_weights=False)\n",
    "model = train_res_unfreeze['model']\n",
    "\n",
    "# evaluate the model\n",
    "model.eval()\n",
    "corrects = 0\n",
    "total = 0\n",
    "with tqdm(total=len(test_loader), desc=f\"Evaluating\") as pbar:\n",
    "    with torch.no_grad():\n",
    "        for sample in test_loader:\n",
    "            image, label = sample\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            outputs = model(image)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            corrects += torch.sum(preds == label.data)\n",
    "            total += image.size(0)\n",
    "            pbar.update(1)\n",
    "print(f\"Test Accuracy: {corrects.double()/total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation accuracy\n",
    "plt.plot(train_res_freeze['train_accuracy'], label='Train accuracy (Frozen weights)')\n",
    "plt.plot(train_res_freeze['val_accuracy'], label='Validation accuracy (Frozen weights)')\n",
    "plt.legend()\n",
    "plt.savefig('train_val_acc.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(train_res_unfreeze['train_accuracy'], label='Train accuracy (Unfrozen weights)')\n",
    "plt.plot(train_res_unfreeze['val_accuracy'], label='Validation accuracy (Unfrozen weights)')\n",
    "plt.legend()\n",
    "plt.savefig('train_val_acc_unfreeze.png')\n",
    "plt.close()\n",
    "\n",
    "# combine frozen and unfrozen training accuracy\n",
    "train_accuracy = train_res_freeze['train_accuracy'] + train_res_unfreeze['train_accuracy']\n",
    "val_accuracy = train_res_freeze['val_accuracy'] + train_res_unfreeze['val_accuracy']\n",
    "\n",
    "plt.plot(train_accuracy, label='Train accuracy')\n",
    "plt.plot(val_accuracy, label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('train_val_acc_combined.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilinear pooling with einops\n",
    "# x1 = rearrange(x1, 'b k h w -> b k (h w)')\n",
    "# x2 = rearrange(x2, 'b k h w -> b k (h w)')\n",
    "# x = einsum('b i j, b k j -> b i k', x1, x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
