{"cells":[{"cell_type":"markdown","metadata":{"id":"L9SrtsdsJoe2"},"source":["# Tensor manipulation using einops\n","\n","In this notebook we will use einops to rewrite typical deep learning operations in a more clear and concise way. First, let's intall einops:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fn_6v0LeKsNh","outputId":"b167dca2-6bf9-4811-878d-7ef0251a58d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n","Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n"]}],"source":["!pip install einops\n","\n","import torch\n","import einops\n","from einops import rearrange, reduce, einsum"]},{"cell_type":"markdown","metadata":{"id":"VWNlajeyLJU7"},"source":["## Flattening a tensor\n","\n","This operation is used typically before fully connected layers.\n","\n","The size of input tensor will be b c h w (batch of images). And the size ouput should be b (c h w)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0QWTcYSvK5e9","outputId":"05a4f834-a0f7-4d06-8fcb-a2adbef45671"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 126])\n"]}],"source":["x = torch.randn(4,3,6,7)\n","\n","# write the einops operation here\n","y = rearrange(x, 'b c h w -> b (c h w)')\n","\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"1NoYVDaqMqxR"},"source":["## Pooling\n","\n","Pooling is typically used to reduce the spatial size of a tensor in convolutional networks.\n","\n","Rewrite pooling2d using einops:"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLCQSxK6MnGV","outputId":"34f20fdb-ba9f-4c6a-a8f4-76dade822261"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 3, 5, 5])\n","the result of difference should be close to zero:\n","tensor(1.1921e-07)\n"]}],"source":["x = torch.randn(4,3,10,10)\n","\n","import  torch.nn.functional as F\n","\n","y = F.avg_pool2d(x,(2,2),2)\n","print(y.shape)\n","\n","#repeat average pooling using einops\n","y2 = reduce(x, 'b c (h1 h2) (w1 w2) -> b c h1 w1', 'mean', h2=2, w2=2)\n","\n","print(\"the result of difference should be close to zero:\")\n","print((y-y2).abs().max())\n"]},{"cell_type":"markdown","metadata":{"id":"Uq3ZIDknPQoe"},"source":["## Patch embedding\n","\n","Patch embedding is the first operation in vision transformers. The steps are:\n","\n","1. Split image into patches\n","2. Flatten patches\n","3. Apply a linear transformation\n","\n","In many implementations, patch embedding is done using a convolutional layer as follows:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"x5mW3a53NwbN"},"outputs":[],"source":["from torch import nn\n","from torch import Tensor\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\n","        self.patch_size = patch_size\n","        super().__init__()\n","        self.projection =  nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n","\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        x = self.projection(x)\n","        # reshape from b embed_size h//patch_size w//patch_size to b ( h//patch_size w//patch_size) embed_size\n","        x = x.view(x.shape[0], x.shape[1], -1) # these two lines can be done with einops too!\n","        x = x.transpose(-2,-1)\n","\n","        return x"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJ4utoXwREzt","outputId":"d4131e30-ccb6-46a9-cc3d-c7b83a3d686f"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 196, 768])\n"]}],"source":["x = torch.randn(4,3,224,224)\n","\n","pe = PatchEmbedding(3,16,768)\n","\n","y = pe(x)\n","\n","print(y.shape)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"4drud9jKRIr5"},"outputs":[],"source":["from einops.layers.torch import Rearrange\n","\n","class PatchEmbedding_einops(nn.Module):\n","    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\n","        self.patch_size = patch_size\n","        super().__init__()\n","        #extract and flatten patches using Rearrange Layer\n","        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size)\n","        self.projection =  nn.Linear(in_channels * patch_size * patch_size, emb_size)\n","\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        x = self.rearrange(x)\n","        x = self.projection(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MBg1LyMZST8q","outputId":"84dac477-971f-4412-94fe-abc8fbd944fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 196, 768])\n"]}],"source":["x = torch.randn(4,3,224,224)\n","\n","pe_einops = PatchEmbedding_einops(3,16,768)\n","\n","y = pe_einops(x)\n","\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"NUTf7O8gT3Z5"},"source":["# Self attention\n","\n","Lets implement a self attention block using basic torch operations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34lwKw-rSaC9","outputId":"98f2d03f-19bc-48c0-bc53-4d7770ecf9f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5, 100, 768])\n"]}],"source":["class SelfAttention(nn.Module):\n","    def __init__(self, input_dim: int = 768, dropout: float = 0):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        # fuse the queries, keys and values in one matrix (more efficient)\n","        self.qkv = nn.Linear(input_dim, input_dim * 3)\n","        self.att_drop = nn.Dropout(dropout)\n","        self.projection = nn.Linear(input_dim, input_dim)\n","\n","    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n","        x_qkv =self.qkv(x)\n","        # split keys, queries and values from x_qkv\n","        queries, keys, values = torch.chunk(x_qkv,3, dim=-1)\n","\n","\n","        scaling = self.input_dim ** (1/2)\n","        energy = torch.bmm(queries, keys.transpose(-1,-2)) #bactched matrix multiplication\n","        if mask is not None:\n","          fill_value = torch.finfo(torch.float32).min\n","          energy.mask_fill(~mask, fill_value)\n","\n","        att = F.softmax(energy, dim = -1) / scaling\n","        att = self.att_drop(att)\n","\n","        out = torch.bmm(att, values)  #bactched matrix multiplication\n","\n","        return out\n","\n","\n","x = torch.randn(5,100,768)\n","sa = SelfAttention(768,0.0)\n","\n","y = sa(x)\n","print(y.shape)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HMfLiIn3Yh8P"},"source":["Now rewrite SelfAttention using einops"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jmDXtZEfVkXZ","outputId":"58ce1f47-7c39-48f1-8246-23521903fe1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5, 100, 768]) torch.Size([5, 100, 768]) torch.Size([5, 100, 768])\n"]},{"ename":"ValueError","evalue":"The last argument passed to `einops.einsum` must be a string, representing the einsum pattern.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m768\u001b[39m)\n\u001b[1;32m     36\u001b[0m sa \u001b[38;5;241m=\u001b[39m SelfAttention_einops(\u001b[38;5;241m768\u001b[39m,\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n","File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[21], line 22\u001b[0m, in \u001b[0;36mSelfAttention_einops.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# calculate energy using einsum\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m energy \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb q d, b d k -> b q k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m   fill_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mmin\n","File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/einops/einops.py:902\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*tensors_and_pattern)\u001b[0m\n\u001b[1;32m    900\u001b[0m pattern \u001b[38;5;241m=\u001b[39m tensors_and_pattern[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pattern, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 902\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe last argument passed to `einops.einsum` must be a string, representing the einsum pattern.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m     )\n\u001b[1;32m    905\u001b[0m tensors \u001b[38;5;241m=\u001b[39m tensors_and_pattern[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    906\u001b[0m pattern \u001b[38;5;241m=\u001b[39m _compactify_pattern_for_einsum(pattern)\n","\u001b[0;31mValueError\u001b[0m: The last argument passed to `einops.einsum` must be a string, representing the einsum pattern."]}],"source":["\n","class SelfAttention_einops(nn.Module):\n","    def __init__(self, input_dim: int = 768, dropout: float = 0):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        # fuse the queries, keys and values in one matrix (more efficient)\n","        self.qkv = nn.Linear(input_dim, input_dim * 3)\n","        self.att_drop = nn.Dropout(dropout)\n","        self.projection = nn.Linear(input_dim, input_dim)\n","\n","    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n","        x_qkv =self.qkv(x)\n","        # split keys, queries and values from x_qkv\n","        # Reshape to 3 b n embed_dim using einops\n","        x_qkv = rearrange(x_qkv, 'b n (qkv e) -> qkv b n e', qkv=3)\n","\n","        queries, keys, values = x_qkv[0], x_qkv[1], x_qkv[2]\n","\n","        print(queries.shape, keys.shape, values.shape)\n","\n","        scaling = self.input_dim ** (1/2)\n","        # calculate energy using einsum\n","        energy = einsum('b q d, b d k -> b q k', queries, keys)\n","        if mask is not None:\n","          fill_value = torch.finfo(torch.float32).min\n","          energy.mask_fill(mask==1, fill_value)\n","\n","        att = F.softmax(energy, dim = -1) / scaling\n","        att = self.att_drop(att)\n","\n","        #multiply att with values using einops\n","        out = None\n","        return out\n","\n","\n","x = torch.randn(5,100,768)\n","sa = SelfAttention_einops(768,0.0)\n","\n","y = sa(x)\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"VC7Jd3V6dUgI"},"source":["## Multihead attention\n","\n","Now let's write multihead attention"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n_BiZCKKZ0IK","outputId":"5da7952b-4005-4694-d804-09e6617d7349"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5, 100, 768])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, input_dim, num_heads, dropout=0.0):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        assert input_dim % num_heads == 0, \"Input dimension must be divisible by the number of heads\"\n","\n","        self.input_dim = input_dim\n","        self.num_heads = num_heads\n","        self.head_dim = input_dim // num_heads\n","\n","        self.qkv = nn.Linear(input_dim, 3*input_dim)\n","\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.output_projection = nn.Linear(input_dim, input_dim)\n","\n","    def forward(self, x, mask=None):\n","        batch_size, seq_len, input_dim = x.size()\n","\n","        x_qkv = self.qkv(x) # only one linear layer\n","                            # x_qkv contains queries, keys values for all heads\n","\n","        # split in querys, keys, values\n","        queries, keys, values = torch.chunk(x_qkv,3, dim=-1)\n","\n","        # Reshape queries, keys, and values to split heads, the ouput dim is:  b num_heads seq_len head_dim\n","        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        values = values.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","        # Compute scaled dot-product attention\n","        scaling = self.input_dim ** (1/2)\n","        energy = torch.matmul(queries, keys.transpose(-2, -1)) / scaling\n","        if mask is not None:\n","            fill_value = torch.finfo(torch.float32).min\n","            energy.mask_fill(~mask, fill_value)\n","\n","        att = F.softmax(energy, dim=-1)\n","        att = self.dropout(att)\n","\n","        # Apply attention to values\n","        out = torch.matmul(att, values)\n","\n","        #importat detail: contigous is required before view (or use reshape!)\n","        out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, input_dim)\n","\n","\n","        return out\n","\n","\n","x = torch.randn(5,100,768)\n","msa = MultiHeadSelfAttention(768,12)\n","\n","y = msa(x)\n","print(y.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"ByZAPntCmZkS"},"source":["Now reimplement the previous class using einops"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XMc8q_bBjymW","outputId":"b12c8d9f-5710-4c9d-9cae-8a4cd562bef5"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5, 100, 768])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MultiHeadSelfAttention_einops(nn.Module):\n","    def __init__(self, input_dim, num_heads, dropout=0.0):\n","        super(MultiHeadSelfAttention_einops, self).__init__()\n","        assert input_dim % num_heads == 0, \"Input dimension must be divisible by the number of heads\"\n","\n","        self.input_dim = input_dim\n","        self.num_heads = num_heads\n","        self.head_dim = input_dim // num_heads\n","\n","        self.qkv = nn.Linear(input_dim, 3*input_dim)\n","\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.output_projection = nn.Linear(input_dim, input_dim)\n","\n","    def forward(self, x, mask=None):\n","        batch_size, seq_len, input_dim = x.size()\n","\n","        x_qkv = self.qkv(x) # only one linear layer\n","                            # x_qkv contains queries, keys values for all heads\n","\n","        # split in querys, keys, values using rearrange as before\n","        x_qkv =\n","\n","        queries, keys, values = x_qkv[0], x_qkv[1], x_qkv[2]\n","\n","\n","\n","        # Reshape queries, keys, and values to split heads, the ouput dim is:  b num_heads seq_len head_dim\n","        queries =\n","        keys =\n","        values =\n","\n","        # Compute scaled dot-product attention usin einops\n","        scaling = self.input_dim ** (1/2)\n","        energy =\n","        if mask is not None:\n","            fill_value = torch.finfo(torch.float32).min\n","            energy.mask_fill(~mask, fill_value)\n","\n","        att = F.softmax(energy, dim=-1)\n","        att = self.dropout(att)\n","\n","        # Apply attention to values usin einops\n","        out =\n","\n","        # rearrange out to join heads\n","        out =\n","\n","\n","\n","        return out\n","\n","\n","x = torch.randn(5,100,768)\n","msa = MultiHeadSelfAttention_einops(768,12)\n","\n","y = msa(x)\n","print(y.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYnYIreukGSy","outputId":"66a152b3-839a-4963-fcd8-fc9b256ebaac"},"outputs":[{"data":{"text/plain":["64.0"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dWDC7QqlImi"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1Sko4xooe2_FVnY1x3jnOW2HBOqOb-FAg","timestamp":1718202201467}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
