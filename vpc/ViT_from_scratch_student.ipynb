{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1QmznK5lq5jvS5UgaefNchWvhzrkAX96V","timestamp":1718202282866}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rTg7cf3ksCUP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8839b75-4776-4960-daa0-0f8985413b9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"]}],"source":["#Requirements\n","!pip install einops"]},{"cell_type":"markdown","source":["# ViT implementation from scratch\n","\n","In this exercise we will implement ViT transformer from scratch"],"metadata":{"id":"Wy9qni7IsGdS"}},{"cell_type":"markdown","source":["## Patch embedding\n","\n","In the previous exercise we have seen the basics of path embedding. To\n","finish the implementation we need to add:\n","1. Class token\n","2. Positional encodings\n","\n","Complete the following code using the rearrange part of the previous exercise\n"],"metadata":{"id":"qSUBeKXKtDsT"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch import Tensor\n","from einops.layers.torch import Rearrange\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n","        self.patch_size = patch_size\n","        super().__init__()\n","        self.projection = nn.Sequential(\n","            # break-down the image in s1 x s2 patches and flat them using rearrange an linear layer\n","\n","            #Put code here!!\n","\n","        )\n","        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n","        #remember that the class token has also an associated positional encoding\n","        self.positional_enc = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","\n","        b, _, _, _ = x.shape\n","        x = self.projection(x)\n","\n","        #repeat the class token b times 1 1 embed_size -> b 1 emb_size\n","        #use torch repeat function\n","        cls_tokens =\n","\n","        #concatenate c and x tensors on the seq dimension (use torch.cat)\n","        #the shape of the result will be b (seq_len +1) emb_size\n","\n","        x =\n","\n","        #add posional encodings\n","\n","        x =\n","\n","        return x\n"],"metadata":{"id":"5hSunrv6sfZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#let's try:\n","img = torch.randn(4,3,224,224)\n","\n","pe = PatchEmbedding(in_channels=3, patch_size=16,emb_size = 768, img_size = 224)\n","\n","print(pe(img).shape) #shoud be [4, 197, 768]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PS0eQP1Ys3wB","outputId":"85f445d6-f956-4436-91f5-77151dfc43ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 197, 768])\n"]}]},{"cell_type":"markdown","source":["## Multihead attention"],"metadata":{"id":"bQDnBRpTzcwV"}},{"cell_type":"code","source":["from einops import rearrange, einsum\n","import torch.nn.functional as F\n","\n","class MultiHeadAttention(nn.Module):\n","  #copy the code of Multihead attention from previous exercise\n","\n","    def __init__(self, emb_size: int = 512, num_heads: int = 8, dropout: float = 0):\n","        super().__init__()\n","        #code here\n","\n","\n","    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n","        # split keys, queries and values in num_heads\n","\n","\n","\n","        return out"],"metadata":{"id":"HxpLfd5Bv_OF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" ## Residual Add\n","\n"," Residual connections are essential in transformers. The following code wraps redidual connection.\n","\n"," We will use this block to wrap residual connections of multihead attention and MLP layers."],"metadata":{"id":"3ZEw-w5H0PYB"}},{"cell_type":"code","source":["class ResidualAdd(nn.Module):\n","    def __init__(self, fn):\n","        super().__init__()\n","        self.fn = fn # torch nn module that is wrapped\n","\n","    def forward(self, x, **kwargs):\n","        res = x\n","        x = self.fn(x, **kwargs)\n","        x += res\n","        return x"],"metadata":{"id":"uTyDsji60OB3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## MLP block\n","\n","This block follows multihead attention, and is a sequential model with:\n","\n","1. Linear layer (with expansion of dimensionality)\n","2. Non linearity (GELU)\n","3. Dropout\n","4. Linear layer that returns to input dimension\n","\n","This block can be very easily implemented by subclassing the nn.Sequential class. Remember that nn.Sequential\n","can be initialized with the list of blocks.\n","\n","This implementation avoids to reimplement the forward method!\n","\n"],"metadata":{"id":"GXM3C5hb1Bdj"}},{"cell_type":"code","source":["# MLP Block\n","\n","class FeedForwardBlock(nn.Sequential):\n","    def __init__(self, emb_size: int, expansion: int = 4, dropout: float = 0.):\n","        #write the four blocks in the init function as sequential\n","        # 1 linear with expansion of dims\n","        # 2 gelu\n","        # 3 dropout\n","        # 3 linear that returns to emb_size\n","\n","\n","        super().__init__(\n","            #put layers here!\n","        )"],"metadata":{"id":"VsasYNEIzwDJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformer encoder block\n","\n","Again we implement this block from the Sequential module. The modules are:\n","\n","1. Block that includes layernorm, mha, wrapped with residual connection\n","2. Block that includes layernorn, FF, wrapped with residual connection\n","\n","\n"],"metadata":{"id":"W5m-_bCa2acP"}},{"cell_type":"code","source":["class TransformerEncoderBlock(nn.Sequential):\n","    def __init__(self,\n","                 emb_size: int = 768,\n","                 forward_expansion: int = 4,\n","                 dropout: float = 0.,\n","                 num_heads: int =  8,\n","                 ):\n","\n","        block_msa = nn.Sequential(\n","                #layer norm\n","                # multihead attention\n","        )\n","\n","        block_ff = nn.Sequential(\n","                #layer norm\n","                # feed forward block\n","            )\n","\n","        #check how residual connecion are done!! :-)\n","        super().__init__(\n","            ResidualAdd(block_msa),\n","            ResidualAdd(block_ff)\n","            )"],"metadata":{"id":"BLypdySs2Tpz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformer encoder\n","\n","This is just a Sequential block of TransformerEncoderBlocks\n","All TransformerEncoderBlock are initialized with the same parameters"],"metadata":{"id":"ltWiHasY45QZ"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Sequential):\n","    def __init__(self, depth: int = 12,\n","                 emb_size: int = 768,\n","                 forward_expansion: int = 4,\n","                 dropout: float = 0.,\n","                 num_heads: int =  8):\n","\n","        # generate list of transformer blocks\n","        transformer_blocks = [TransformerEncoderBlock(emb_size=emb_size,\n","                                                      forward_expansion=forward_expansion,\n","                                                      dropout = dropout,\n","                                                      num_heads = num_heads) for _ in range(depth)]\n","\n","\n","        super().__init__(*transformer_blocks)"],"metadata":{"id":"rX2Xwtzg44xA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Classification Head\n","\n","Although originally the class token was the only one used for classification, it is a common practice to average all tokens and then apply the linear transform to the number of classes"],"metadata":{"id":"IhE0oG8s9aJZ"}},{"cell_type":"code","source":["from einops.layers.torch import Reduce\n","\n","class ClassificationHead(nn.Sequential):\n","    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n","        super().__init__(\n","            Reduce('b n e -> b e', reduction='mean'),\n","            nn.LayerNorm(emb_size),\n","            nn.Linear(emb_size, n_classes))\n","\n","\n","\n","x = torch.rand(10,14*14+1, 768)\n","\n","ch = ClassificationHead()\n","\n","logits = ch(x)\n","print(logits.shape) # 10 x 1000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vV3tIEr64yYN","outputId":"0d1bf8f6-af57-4832-a1cf-74d5253d45c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 1000])\n"]}]},{"cell_type":"markdown","source":["## Putting all together: ViT transformer"],"metadata":{"id":"FGfxfqE6-T35"}},{"cell_type":"code","source":["class ViT(nn.Sequential):\n","    def __init__(self,\n","                in_channels: int = 3,\n","                patch_size: int = 16,\n","                emb_size: int = 768,\n","                img_size: int = 224,\n","                depth: int = 12,\n","                num_heads: int = 8,\n","                n_classes: int = 1000,\n","                ):\n","        super().__init__(\n","            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n","            TransformerEncoder(depth, emb_size=emb_size, num_heads=num_heads),\n","            ClassificationHead(emb_size, n_classes)\n","        )\n"],"metadata":{"id":"gayC-Gsg-xJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vit = ViT()\n","\n","x = torch.rand(10,3,224,224)\n","\n","logits = vit(x)\n","\n","print(logits.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N3FR0KOl_FgG","outputId":"155735d8-46f8-4f08-a73f-51b979e74459"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 1000])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WnWRpL_H_N2u"},"execution_count":null,"outputs":[]}]}